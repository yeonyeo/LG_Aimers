{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lrCkQbSwhnovW3VvUfOWInZQrOl5IDiu",
      "authorship_tag": "ABX9TyON//wQDDnZZDM0WwvqtEnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeonyeo/LG_Aimers/blob/main/test1_RandomizedSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOd1EfoXC45n",
        "outputId": "bbc557e2-1c9d-4d56-9e46-afe8933ea686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (19.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "GB9K3xS-C9-r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOOKBACK, PREDICT, BATCH_SIZE, EPOCHS = 28, 7, 16, 50\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "4Bd5I5PDDAdF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/open/train/train.csv')"
      ],
      "metadata": {
        "id": "WAmhua1sDDcb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "print(train.isna().sum())\n",
        "\n",
        "# 음수 판매량 개수 확인\n",
        "print(\"음수 매출 개수:\", (train['매출수량']<0).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBlFERwKDF3m",
        "outputId": "caf8f6d7-3223-4b85-df93-d24a6a57e9a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영업일자        0\n",
            "영업장명_메뉴명    0\n",
            "매출수량        0\n",
            "dtype: int64\n",
            "음수 매출 개수: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 음수 이상치 0으로 처리\n",
        "train['매출수량'] = train['매출수량'].clip(lower=0)"
      ],
      "metadata": {
        "id": "U1KNEl8eDHxp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "print(train.isna().sum())\n",
        "\n",
        "# 음수 판매량 개수 확인\n",
        "print(\"음수 매출 개수:\", (train['매출수량']<0).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noZboI5JDJfJ",
        "outputId": "ecf10a96-3e20-4623-c8ef-39ea2ac5d505"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영업일자        0\n",
            "영업장명_메뉴명    0\n",
            "매출수량        0\n",
            "dtype: int64\n",
            "음수 매출 개수: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# 카테고리 인코딩\n",
        "# 업장명, 메뉴명 분리\n",
        "train[['업장명', '메뉴명']] = train['영업장명_메뉴명'].str.split('_', expand=True)\n",
        "# 각각 인코딩\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "train['업장명'] = le1.fit_transform(train['업장명'])\n",
        "train['메뉴명'] = le2.fit_transform(train['메뉴명'])"
      ],
      "metadata": {
        "id": "2ZJmQrnpDLIV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일링\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "train[['매출수량']] = scaler.fit_transform(train[['매출수량']])"
      ],
      "metadata": {
        "id": "JLG4F-s5DNOF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처엔지니링\n",
        "train['영업일자'] = pd.to_datetime(train['영업일자'])\n",
        "# 요일 컬럼 생성\n",
        "train['요일'] = pd.to_datetime(train['영업일자']).dt.dayofweek  # 0=월~6=일\n",
        "# 주말 플래그\n",
        "train['is_weekend'] = train['요일'].isin([5, 6]).astype(int)\n",
        "# 금요일, 토요일, 일요일 개별 플래그\n",
        "train['is_friday'] = (train['요일'] == 4).astype(int)\n",
        "train['is_saturday'] = (train['요일'] == 5).astype(int)\n",
        "train['is_sunday'] = (train['요일'] == 6).astype(int)\n",
        "# 공휴일 플래그\n",
        "holidays = ['2023-05-05', '2023-08-15', '2023-09-28', ...]  # 직접 입력\n",
        "train['is_holiday'] = train['영업일자'].astype(str).isin(holidays).astype(int)\n",
        "\n",
        "# 월, 계절\n",
        "train['월'] = train['영업일자'].dt.month\n",
        "train['계절'] = train['월'].map({12:'겨울', 1:'겨울', 2:'겨울', 3:'봄', 4:'봄', 5:'봄', 6:'여름', 7:'여름', 8:'여름', 9:'가을', 10:'가을', 11:'가을'})\n",
        "train = pd.get_dummies(train, columns=['계절'])\n",
        "# 최근 N일(7, 14, 28)간 매출 평균\n",
        "for window in [7, 14, 28]:\n",
        "    train[f'rolling_mean_{window}'] = train.groupby('영업장명_메뉴명')['매출수량']\\\n",
        "        .transform(lambda x: x.rolling(window, min_periods=1).mean().shift(1))"
      ],
      "metadata": {
        "id": "bmKHsSAPDO6t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "for col in ['영업장명_메뉴명', '요일']:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = le.fit_transform(train[col])"
      ],
      "metadata": {
        "id": "L3LQz7JcDQ_N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_day = train['영업일자'].sort_values().unique()[-7]  # 마지막 7일 기준\n",
        "train_set = train[train['영업일자'] < split_day]\n",
        "valid_set = train[train['영업일자'] >= split_day]\n",
        "\n",
        "X_train = train_set.drop(['매출수량', '영업일자'], axis=1)\n",
        "y_train = train_set['매출수량']\n",
        "X_valid = valid_set.drop(['매출수량', '영업일자'], axis=1)\n",
        "y_valid = valid_set['매출수량']"
      ],
      "metadata": {
        "id": "8KpmjNC8DSrA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NaN 처리\n",
        "train = train.fillna(0)\n",
        "# 숫자형만 남기고 결측값 채우기\n",
        "X_train = X_train.select_dtypes(include=[np.number]).fillna(0)\n",
        "X_valid = X_valid.select_dtypes(include=[np.number]).fillna(0)"
      ],
      "metadata": {
        "id": "IPAdgQslDUs_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# 파라미터 후보군 (적당히 좁게, n_iter=20 정도부터 시작)\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 300, 500, 700, 1000],\n",
        "    'max_depth': [4, 6, 8, 10, 12],\n",
        "    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
        "    'reg_alpha': [0, 0.01, 0.05, 0.1, 0.5],\n",
        "    'reg_lambda': [0, 0.01, 0.05, 0.1, 0.5],\n",
        "}\n",
        "\n",
        "lgbm = LGBMRegressor(random_state=42)\n",
        "random_search = RandomizedSearchCV(\n",
        "    lgbm,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,              # 20회만 시도, Colab이면 n_iter=10~20 추천\n",
        "    scoring='neg_mean_absolute_error',  # 회귀면 MAE가 빠르고 실용적\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "print(\"Best Score (neg MAE):\", random_search.best_score_)\n",
        "# 평가\n",
        "def smape(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    mask = y_true != 0\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    smape_vals = np.abs(y_true - y_pred) / denominator\n",
        "    return np.mean(smape_vals) * 100\n",
        "# 튜닝된 모델로 검증셋 예측\n",
        "best_lgbm = random_search.best_estimator_\n",
        "lgbm_pred = best_lgbm.predict(X_valid)\n",
        "print(\"튜닝 후 LightGBM SMAPE:\", smape(y_valid, lgbm_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08wolLA5DWh7",
        "outputId": "f78fd316-f2b7-4fd4-89f3-167c438d2f2e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005146 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1173\n",
            "[LightGBM] [Info] Number of data points in the train set: 101325, number of used features: 13\n",
            "[LightGBM] [Info] Start training from score 0.007801\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best Params: {'subsample': 0.7, 'reg_lambda': 0.5, 'reg_alpha': 0.5, 'n_estimators': 700, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
            "Best Score (neg MAE): -0.006048595046891985\n",
            "튜닝 후 LightGBM SMAPE: 66.29621524987893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 300, 500, 700, 1000],\n",
        "    'max_depth': [4, 6, 8, 10, 12],\n",
        "    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
        "    'reg_alpha': [0, 0.01, 0.05, 0.1, 0.5],\n",
        "    'reg_lambda': [0, 0.01, 0.05, 0.1, 0.5],\n",
        "}\n",
        "\n",
        "xgb = XGBRegressor(tree_method='hist', random_state=42)\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "xgb_pred = random_search.best_estimator_.predict(X_valid)\n",
        "print(\"튜닝 후 XGBoost SMAPE:\", smape(y_valid, xgb_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxX8wlcKDZFA",
        "outputId": "bdc144e4-634e-4370-b35b-475c326a7d72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Best Params: {'subsample': 0.9, 'reg_lambda': 0.1, 'reg_alpha': 0.5, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.03, 'colsample_bytree': 1.0}\n",
            "튜닝 후 XGBoost SMAPE: 67.14715924293961\n"
          ]
        }
      ]
    }
  ]
}